{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9hFiYURGuZX",
        "outputId": "f429bc4d-85cf-4887-fada-4a8c3ff415d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "8B-fyBRWHBX0"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkJa5EXeHMhm",
        "outputId": "b901950b-d9a9-441b-c724-a745e5754082"
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> l\n",
            "Packages:\n",
            "  [ ] abc................. Australian Broadcasting Commission 2006\n",
            "  [ ] alpino.............. Alpino Dutch Treebank\n",
            "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
            "  [ ] averaged_perceptron_tagger_eng Averaged Perceptron Tagger (JSON)\n",
            "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
            "  [ ] averaged_perceptron_tagger_rus Averaged Perceptron Tagger (Russian)\n",
            "  [ ] basque_grammars..... Grammars for Basque\n",
            "  [ ] bcp47............... BCP-47 Language Tags\n",
            "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
            "                           Extraction Systems in Biology)\n",
            "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
            "  [ ] book_grammars....... Grammars from NLTK Book\n",
            "  [ ] brown............... Brown Corpus\n",
            "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
            "  [ ] cess_cat............ CESS-CAT Treebank\n",
            "  [ ] cess_esp............ CESS-ESP Treebank\n",
            "  [ ] chat80.............. Chat-80 Data Files\n",
            "  [ ] city_database....... City Database\n",
            "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
            "  [ ] comparative_sentences Comparative Sentence Dataset\n",
            "Hit Enter to continue: comparative_sentences\n",
            "  [ ] comtrans............ ComTrans Corpus Sample\n",
            "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
            "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
            "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
            "                           and Basque Subset)\n",
            "  [ ] crubadan............ Crubadan Corpus\n",
            "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
            "  [ ] dolch............... Dolch Word List\n",
            "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
            "                           Corpus\n",
            "  [ ] extended_omw........ Extended Open Multilingual WordNet\n",
            "  [ ] floresta............ Portuguese Treebank\n",
            "  [ ] framenet_v15........ FrameNet 1.5\n",
            "  [ ] framenet_v17........ FrameNet 1.7\n",
            "  [ ] gazetteers.......... Gazeteer Lists\n",
            "  [ ] genesis............. Genesis Corpus\n",
            "  [ ] gutenberg........... Project Gutenberg Selections\n",
            "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
            "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
            "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
            "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
            "                           ChaSen format)\n",
            "Hit Enter to continue: book_grammars\n",
            "  [ ] kimmo............... PC-KIMMO Data Files\n",
            "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
            "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
            "                           for parser comparison\n",
            "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
            "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
            "                           part-of-speech tags\n",
            "  [ ] machado............. Machado de Assis -- Obra Completa\n",
            "  [ ] masc_tagged......... MASC Tagged Corpus\n",
            "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
            "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
            "  [ ] moses_sample........ Moses Sample Models\n",
            "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
            "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
            "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
            "                           2015) subset of the Paraphrase Database.\n",
            "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
            "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
            "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
            "  [ ] nps_chat............ NPS Chat\n",
            "  [ ] omw-1.4............. Open Multilingual Wordnet\n",
            "Hit Enter to continue: large_grammars\n",
            "  [ ] omw................. Open Multilingual Wordnet\n",
            "  [ ] opinion_lexicon..... Opinion Lexicon\n",
            "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
            "  [ ] paradigms........... Paradigm Corpus\n",
            "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
            "                           Evaluation Shared Task\n",
            "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
            "                           character properties in Perl\n",
            "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
            "  [ ] pl196x.............. Polish language of the XX century sixties\n",
            "  [ ] porter_test......... Porter Stemmer Test Files\n",
            "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
            "  [ ] problem_reports..... Problem Report Corpus\n",
            "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
            "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
            "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
            "  [ ] pros_cons........... Pros and Cons\n",
            "  [ ] ptb................. Penn Treebank\n",
            "  [ ] punkt............... Punkt Tokenizer Models\n",
            "  [ ] qc.................. Experimental Data for Question Classification\n",
            "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
            "                           version\n",
            "Hit Enter to continue: punkt\n",
            "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
            "                           Portuguesa)\n",
            "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
            "  [ ] sample_grammars..... Sample Grammars\n",
            "  [ ] semcor.............. SemCor 3.0\n",
            "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
            "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
            "  [ ] sentiwordnet........ SentiWordNet\n",
            "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
            "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
            "  [ ] smultron............ SMULTRON Corpus Sample\n",
            "  [ ] snowball_data....... Snowball Data\n",
            "  [ ] spanish_grammars.... Grammars for Spanish\n",
            "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
            "  [ ] stopwords........... Stopwords Corpus\n",
            "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
            "  [ ] swadesh............. Swadesh Wordlists\n",
            "  [ ] switchboard......... Switchboard Corpus Sample\n",
            "  [ ] tagsets............. Help on Tagsets\n",
            "  [ ] tagsets_json........ Help on Tagsets\n",
            "  [ ] timit............... TIMIT Corpus Sample\n",
            "Hit Enter to continue: sample_grammars\n",
            "  [ ] toolbox............. Toolbox Sample Files\n",
            "  [ ] treebank............ Penn Treebank Sample\n",
            "  [ ] twitter_samples..... Twitter Samples\n",
            "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
            "                           (Unicode Version)\n",
            "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
            "  [ ] unicode_samples..... Unicode Samples\n",
            "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
            "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
            "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
            "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
            "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
            "  [ ] webtext............. Web Text Corpus\n",
            "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
            "  [ ] word2vec_sample..... Word2Vec Sample\n",
            "  [ ] wordnet2021......... Open English Wordnet 2021\n",
            "  [ ] wordnet2022......... Open English Wordnet 2022\n",
            "  [ ] wordnet31........... Wordnet 3.1\n",
            "  [ ] wordnet............. WordNet\n",
            "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
            "  [ ] words............... Word Lists\n",
            "Hit Enter to continue: toolbox\n",
            "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
            "                           English Prose\n",
            "\n",
            "Collections:\n",
            "  [ ] all-corpora......... All the corpora\n",
            "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
            "                           branch\n",
            "  [ ] all................. All packages\n",
            "  [ ] book................ Everything used in the NLTK Book\n",
            "  [ ] popular............. Popular packages\n",
            "  [ ] tests............... Packages for running tests\n",
            "  [ ] third-party......... Third-party data packages\n",
            "\n",
            "([*] marks installed packages)\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> popular\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading collection 'popular'\n",
            "       | \n",
            "       | Downloading package cmudict to /root/nltk_data...\n",
            "       |   Unzipping corpora/cmudict.zip.\n",
            "       | Downloading package gazetteers to /root/nltk_data...\n",
            "       |   Unzipping corpora/gazetteers.zip.\n",
            "       | Downloading package genesis to /root/nltk_data...\n",
            "       |   Unzipping corpora/genesis.zip.\n",
            "       | Downloading package gutenberg to /root/nltk_data...\n",
            "       |   Unzipping corpora/gutenberg.zip.\n",
            "       | Downloading package inaugural to /root/nltk_data...\n",
            "       |   Unzipping corpora/inaugural.zip.\n",
            "       | Downloading package movie_reviews to /root/nltk_data...\n",
            "       |   Unzipping corpora/movie_reviews.zip.\n",
            "       | Downloading package names to /root/nltk_data...\n",
            "       |   Unzipping corpora/names.zip.\n",
            "       | Downloading package shakespeare to /root/nltk_data...\n",
            "       |   Unzipping corpora/shakespeare.zip.\n",
            "       | Downloading package stopwords to /root/nltk_data...\n",
            "       |   Unzipping corpora/stopwords.zip.\n",
            "       | Downloading package treebank to /root/nltk_data...\n",
            "       |   Unzipping corpora/treebank.zip.\n",
            "       | Downloading package twitter_samples to /root/nltk_data...\n",
            "       |   Unzipping corpora/twitter_samples.zip.\n",
            "       | Downloading package omw to /root/nltk_data...\n",
            "       | Downloading package omw-1.4 to /root/nltk_data...\n",
            "       | Downloading package wordnet to /root/nltk_data...\n",
            "       | Downloading package wordnet2021 to /root/nltk_data...\n",
            "       | Downloading package wordnet31 to /root/nltk_data...\n",
            "       | Downloading package wordnet_ic to /root/nltk_data...\n",
            "       |   Unzipping corpora/wordnet_ic.zip.\n",
            "       | Downloading package words to /root/nltk_data...\n",
            "       |   Unzipping corpora/words.zip.\n",
            "       | Downloading package maxent_ne_chunker to /root/nltk_data...\n",
            "       |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "       | Downloading package punkt to /root/nltk_data...\n",
            "       |   Unzipping tokenizers/punkt.zip.\n",
            "       | Downloading package snowball_data to /root/nltk_data...\n",
            "       | Downloading package averaged_perceptron_tagger to\n",
            "       |     /root/nltk_data...\n",
            "       |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "       | \n",
            "     Done downloading collection popular\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://en.wikipedia.org/wiki/Ajit_Doval\"  # Replace with the URL of the webpage you want to scrape\n",
        "response = requests.get(url)\n",
        "html_content = response.content\n",
        "\n",
        "soup = BeautifulSoup(html_content, 'html.parser')"
      ],
      "metadata": {
        "id": "x9QhZZiOHY4S"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting Text Data\n",
        "### Once we have the parsed HTML, trying to extract the relevant text data using various methods such as .find(), .find_all(), and .get_text()."
      ],
      "metadata": {
        "id": "tqf06USeIrGl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Text Data\n",
        "### Once we have the parsed HTML, trying to extract the relevant text data using various methods such as .find(), .find_all(), and .get_text().# New Section"
      ],
      "metadata": {
        "id": "9M6z9x8pJFS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Extracting all paragraphs\n",
        "paragraphs = soup.find_all('p')\n",
        "\n",
        "# Extracting text from each paragraph\n",
        "paragraph_texts = [paragraph.get_text() for paragraph in paragraphs]"
      ],
      "metadata": {
        "id": "FWgDkXanJe9W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing\n",
        "#### here we are trying to text preprocessing, it involves various steps to clean and normalize the extracted text.\n"
      ],
      "metadata": {
        "id": "znq9Dpd2J1ne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting to lowercase\n",
        "lowercase_text = [text.lower() for text in paragraph_texts]\n",
        "\n",
        "# Removing special characters using regex\n",
        "cleaned_text = [re.sub(r'[^a-zA-Z0-9\\s]', '', text) for text in lowercase_text]\n",
        "\n",
        "# Tokenization\n",
        "tokenized_text = [word_tokenize(text) for text in cleaned_text]\n",
        "\n",
        "# Removing stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_text = [[word for word in tokens if word not in stop_words] for tokens in tokenized_text]\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_text = [[stemmer.stem(word) for word in tokens] for tokens in filtered_text]"
      ],
      "metadata": {
        "id": "6bllsWV6J-vL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4hbHKHltKbPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Processing\n",
        "#### we can perform additional steps such as removing empty tokens, converting the processed text back to sentences or paragraphs, and so on, based on our requirements."
      ],
      "metadata": {
        "id": "ZWJE9UPTKkhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing empty tokens\n",
        "final_text = [[word for word in tokens if word.strip()] for tokens in stemmed_text]\n",
        "\n",
        "# Converting tokens back to sentences\n",
        "sentences = [' '.join(tokens) for tokens in final_text]\n",
        "\n",
        "# Converting sentences back to paragraphs\n",
        "processed_paragraphs = '\\n\\n'.join(sentences)"
      ],
      "metadata": {
        "id": "Hb4WaOosKqJD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z9jCYWlnKxlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving processed Text\n",
        "#### *Finally*, we can save the processed text to a file for further analysis."
      ],
      "metadata": {
        "id": "1NnoY-5YK2cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('processed_text.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(processed_paragraphs)\n",
        "\n",
        "print(processed_paragraphs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH9T8wT0LDZi",
        "outputId": "19da7713-99b6-4dea-948b-8a0b3ae0c3d8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ajit kumar doval kc ppm pm born 20 januari 1945 former spymast current nation secur advisor nsa india3 ajit doval longest serv nsa india present tenur notabl doval third consecut extens sinc 201445 intellig bureau ib chief former indian polic servic ip offic kerala cadre6789\n",
            "\n",
            "previous serv director ib 200405 work decad head oper wing10 work undercov spi ib one year pakistan 6 year offic indian high commiss islamabad11 spent part career spi ib11\n",
            "\n",
            "success oper spi intellig head includ oper black thunder 1988 rescu 46 indian nation iraq 2015 oper versu nagaland milit india armi sabotag terror organis pfi mani more31112\n",
            "\n",
            "founder director right centr lean think tank vivekananda intern foundat vif appoint nsa found decemb 20091314\n",
            "\n",
            "doval born 1945 ghiri banelsyun villag pauri garhwal erstwhil unit provinc uttarakhand doval father major g n doval offic indian army15616\n",
            "\n",
            "receiv earli educ ajmer militari school ajmer rajasthan7 graduat master degre econom agra univers 196716\n",
            "\n",
            "doval join indian polic servic ip 1968 kerala cadr assist superintend polic asp kottayam district1718 when1920\n",
            "\n",
            "activ involv antiinsurg oper punjab21when\n",
            "\n",
            "doval work thalasseri kerala month 1972 join central service22 experi involv termin 15 hijack indian airlin aircraft 1971 199923 headquart head ib oper wing decad founder chairman multi agenc centr mac well joint task forc intellig jtfi24\n",
            "\n",
            "1988 oper black thunder infiltr golden templ pose isi agent espionag khalistani separatist doval gather inform weapon made map posit becam import member group gave wrong advic sabotag help nation secur guard nsg win golden temple2526\n",
            "\n",
            "play role intellig sikkim merger india2728 train k narayanan third nation secur advisor india brief period counterterror operations29 part team sent kandahar afghanistan negoti releas passeng hijack aeroplan ic814273031\n",
            "\n",
            "later appoint post director intellig bureau31\n",
            "\n",
            "alleg 2016 surgic strike pakistan part ajit doval offens defenc strategy32 though pakistan reject india claim said indian troop cross line control skirmish pakistani troop border\n",
            "\n",
            "doval one seven person knew india classifi 2019 balakot airstrik includ indian navi armi airforc chief prime minist narendra modi pakistan base terrorist attack convoy crpf car bomb pulwama martyr 40 india airforc conduct airstrik alleg terrorist base pakistan howev strike hit target doval awak whole night dignitari war room33\n",
            "\n",
            "doval retir januari 2005 director intellig bureau34 decemb 2009 becam found director vivekananda intern foundat vif public polici think tank set vivekananda kendra3536 doval remain activ involv discours nation secur india3738 besid write editori piec sever lead newspap journal deliv lectur india secur challeng foreign polici object sever renown govern nongovernment institut secur thinktank india abroad3940\n",
            "\n",
            "2009 2011 cowrot two report indian black money abroad secret bank tax havens41 other lead field part task forc constitut bharatiya janata parti bjp42\n",
            "\n",
            "2012 ib eye due rule parti congresss suspicion doval think tank vif doubt vif brain behind ramdev anna hazar led anticorrupt movement gener anger government43 mani member vif got appoint top govern posit bjp came power14\n",
            "\n",
            "recent year deliv guest lectur strateg issu iiss london capitol hill washington dc australiaindia institut univers melbourn nation defenc colleg new delhi lal bahadur shastri nation academi administr mussoorie44 doval also spoken intern global event cite everincreas need cooper major establish emerg power world45\n",
            "\n",
            "30 may 2014 doval appoint india fifth nation secur advisor june 2014 doval facilit return 46 indian nurs trap hospit tikrit iraq follow captur mosul isil doval flew iraq 25 june 2014 understand posit ground make highlevel contact iraqi government46 although exact circumst releas unclear 5 juli 2014 isil milit hand nurs kurdish author erbil citi air india plane speciallyarrang indian govern brought back home kochi47\n",
            "\n",
            "along armi chief gener dalbir singh suhag doval plan crossbord militari oper nation socialist council nagaland nscnk separatist oper myanmar indian offici claim mission success 2038 separatist belong nationalist socialist council nagaland nscnk kill operation48495051 howev myanmar govern deni strike accord myanmar offici indian oper nscnk took place entir indian side border5253\n",
            "\n",
            "wide credit doctrin shift indian nation secur polici relat pakistan54 specul septemb 2016 indian strike pakistancontrol kashmir brainchild55565758 doval wide credit along foreign secretari jaishankar indian ambassador china vijay keshav gokhal resolv doklam standoff diplomat channel negotiations596061\n",
            "\n",
            "octob 2018 appoint chairman strateg polici group spg first tier three tier structur nation secur council form nucleu decisionmak apparatus62\n",
            "\n",
            "follow 2019 balakot airstrik retaliatori 2019 jammu kashmir airstrik subsequ captur indian pilot abhinandan varthaman pakistani militari ajit doval held talk us secretari state nation secur advisor secur releas indian pilot63\n",
            "\n",
            "3 june 2019 reappoint nsa 5 year grant person rank cabinet minister64 doval first nsa hold rank wide consid one modi power trust advisor major influenc india nation secur foreign affair\n",
            "\n",
            "also instrument figur revoc special statu jammu kashmir65\n",
            "\n",
            "26 februari 2020 ajit doval walk street riothit northeast delhi assess situat reassur local residents66\n",
            "\n",
            "15 may 2020 militari forc myanmar hand group 22 milit leader activ assam northeast state indian govern made possibl negoti head doval6768\n",
            "\n",
            "15 septemb 2020 doval walk virtual sco meet pakistan project fictiti map omit part india69\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wNOBSP81LKSF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}